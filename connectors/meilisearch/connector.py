"""MeiliSearch Connector for Fivetran - syncs indexes and documents from MeiliSearch API.
This connector demonstrates how to fetch index and document data from MeiliSearch REST API and upsert it into destination using the Fivetran Connector SDK.
See the Technical Reference documentation (https://fivetran.com/docs/connectors/connector-sdk/technical-reference#update)
and the Best Practices documentation (https://fivetran.com/docs/connectors/connector-sdk/best-practices) for details
"""

# For reading configuration from a JSON file
import json

# Import required classes from fivetran_connector_sdk
from fivetran_connector_sdk import Connector

# For enabling Logs in your connector code
from fivetran_connector_sdk import Logging as log

# For supporting Data operations like Upsert(), Update(), Delete() and checkpoint()
from fivetran_connector_sdk import Operations as op

# For making HTTP requests to MeiliSearch API
import requests

# For handling time operations and timestamps
import time

# For type hints in function signatures
from typing import Optional, Dict, List


__DOCUMENTS_ENDPOINT = "/indexes/{index_uid}/documents/fetch"
__INDEXES_ENDPOINT = "/indexes"
__PAGINATION_LIMIT = 100
__REQUEST_TIMEOUT_IN_SECONDS = 30
__MAX_RETRIES = 5
__BACKOFF_BASE = 1
__CHECKPOINT_INTERVAL = 500


def schema(configuration: dict):
    """
    Define the schema function which lets you configure the schema your connector delivers.
    See the technical reference documentation for more details on the schema function:
    https://fivetran.com/docs/connectors/connector-sdk/technical-reference#schema
    Args:
        configuration: a dictionary that holds the configuration settings for the connector.
    """
    return [
        {
            "table": "index",
            "primary_key": ["uid"],
        },
        {
            "table": "document",
            "primary_key": ["_index_uid", "_document_id"],
        },
    ]


def update(configuration: dict, state: dict):
    """
    Define the update function which lets you configure how your connector fetches data.
    See the technical reference documentation for more details on the update function:
    https://fivetran.com/docs/connectors/connector-sdk/technical-reference#update
    Args:
        configuration: a dictionary that holds the configuration settings for the connector.
        state: a dictionary that holds the state of the connector.
    """
    log.warning("Example: Source Examples - MeiliSearch")

    api_url = configuration.get("api_url")
    api_key = configuration.get("api_key")

    headers = build_request_headers(api_key)

    last_sync_timestamp = state.get("last_sync_timestamp")
    synced_indexes = state.get("synced_indexes", [])
    current_sync_timestamp = int(time.time() * 1000)

    # Sync indexes first
    sync_indexes(api_url, headers)

    # Checkpoint after completing the index table sync
    # This ensures we don't lose index sync progress if the sync fails during document processing
    new_state = {
        "last_sync_timestamp": last_sync_timestamp,
        "synced_indexes": synced_indexes,
        "indexes_synced": True,
    }
    op.checkpoint(new_state)
    log.info("Checkpointed after completing index sync")

    # Sync documents from all indexes
    sync_documents_from_all_indexes(api_url, headers, last_sync_timestamp, synced_indexes)

    # Final checkpoint after all syncs are complete
    final_state = {
        "last_sync_timestamp": current_sync_timestamp,
        "synced_indexes": [],
        "indexes_synced": False,
    }
    op.checkpoint(final_state)
    log.info("Checkpointed after completing all syncs")


def build_request_headers(api_key: str) -> Dict[str, str]:
    """
    Build HTTP headers for MeiliSearch API requests.
    Args:
        api_key: The API key for authentication
    Returns:
        Dictionary containing HTTP headers
    """
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def make_api_request(
    url: str,
    headers: Dict[str, str],
    method: str = "GET",
    params: Optional[Dict] = None,
    json_payload: Optional[Dict] = None,
) -> Dict:
    """
    Make an HTTP request to the MeiliSearch API with error handling and exponential backoff retry logic.
    Args:
        url: The API endpoint URL
        headers: HTTP headers for the request
        method: HTTP method (GET or POST)
        params: Optional query parameters
        json_payload: Optional JSON payload for POST requests
    Returns:
        The JSON response from the API
    Raises:
        requests.exceptions.RequestException: For HTTP errors
        ValueError: For invalid JSON responses
    """
    params = params or {}
    json_payload = json_payload or {}

    for attempt in range(__MAX_RETRIES):
        response = None
        try:
            if method == "POST":
                response = requests.post(
                    url,
                    headers=headers,
                    params=params,
                    json=json_payload,
                    timeout=__REQUEST_TIMEOUT_IN_SECONDS,
                )
            else:
                response = requests.get(
                    url, headers=headers, params=params, timeout=__REQUEST_TIMEOUT_IN_SECONDS
                )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout:
            if attempt < __MAX_RETRIES - 1:
                delay = __BACKOFF_BASE * (2**attempt)
                log.warning(f"Request timeout for URL: {url}. Retrying in {delay} seconds...")
                time.sleep(delay)
                continue
            log.severe(f"Request timeout for URL: {url}")
            raise
        except requests.exceptions.HTTPError as e:
            is_retryable_error = response and (
                response.status_code == 429 or response.status_code >= 500
            )
            should_retry = is_retryable_error and attempt < __MAX_RETRIES - 1
            if should_retry:
                delay = __BACKOFF_BASE * (2**attempt)
                log.warning(
                    f"Rate limit or server error for URL: {url}. Retrying in {delay} seconds..."
                )
                time.sleep(delay)
                continue
            log.severe(f"HTTP error for URL: {url}: {e}")
            raise
        except (requests.exceptions.RequestException, ValueError) as e:
            log.severe(f"Request failed for URL: {url}: {e}")
            raise

    raise requests.exceptions.RequestException(
        f"Request failed for URL: {url} after {__MAX_RETRIES} attempts"
    )


def sync_indexes(api_url: str, headers: Dict[str, str]):
    """
    Fetch and sync index metadata from MeiliSearch API.
    Args:
        api_url: The base URL of the MeiliSearch instance
        headers: HTTP headers including authorization
    """
    log.info("Starting indexes sync")

    url = f"{api_url}{__INDEXES_ENDPOINT}"
    params = {"limit": __PAGINATION_LIMIT, "offset": 0}

    indexes_synced = 0

    while True:
        response_data = make_api_request(url, headers, params=params)
        indexes = response_data.get("results", [])

        for index in indexes:
            index_data = {
                "uid": index.get("uid"),
                "created_at": index.get("createdAt"),
                "updated_at": index.get("updatedAt"),
                "primary_key": index.get("primaryKey"),
            }

            # The 'upsert' operation is used to insert or update data in the destination table.
            # The first argument is the name of the destination table.
            # The second argument is a dictionary containing the record to be upserted.
            op.upsert(table="index", data=index_data)
            indexes_synced += 1

        total = response_data.get("total", 0)
        current_offset = params["offset"]
        fetched_count = len(indexes)

        if current_offset + fetched_count >= total:
            break

        params["offset"] += __PAGINATION_LIMIT

    log.info(f"Synced {indexes_synced} indexes")


def sync_documents_from_all_indexes(
    api_url: str,
    headers: Dict[str, str],
    last_sync_timestamp: Optional[int] = None,
    synced_indexes: List[str] = None,
):
    """
    Fetch documents from all indexes in MeiliSearch and sync them to the destination.
    Supports incremental sync when documents have a last_updated field configured as filterable.
    Args:
        api_url: The base URL of the MeiliSearch instance
        headers: HTTP headers including authorization
        last_sync_timestamp: Timestamp in milliseconds for incremental sync filtering
        synced_indexes: List of index UIDs that have already been synced
    """
    log.info("Starting documents sync from all indexes")

    if synced_indexes is None:
        synced_indexes = []

    indexes = fetch_all_indexes(api_url, headers)

    for index in indexes:
        index_uid = index.get("uid")
        if index_uid:
            # Skip indexes that have already been synced (for resumption after failure)
            if index_uid in synced_indexes:
                log.info(f"Skipping already synced index: {index_uid}")
                continue

            sync_documents_for_index(api_url, headers, index_uid, last_sync_timestamp)

            # Checkpoint after each index is fully synced
            # This ensures we can resume from the next index if the sync fails
            synced_indexes.append(index_uid)
            checkpoint_state = {
                "last_sync_timestamp": last_sync_timestamp,
                "synced_indexes": synced_indexes,
                "indexes_synced": True,
            }
            op.checkpoint(checkpoint_state)
            log.info(f"Checkpointed after syncing index: {index_uid}")


def fetch_all_indexes(api_url: str, headers: Dict[str, str]) -> List[Dict]:
    """
    Fetch all indexes from MeiliSearch API.
    Args:
        api_url: The base URL of the MeiliSearch instance
        headers: HTTP headers including authorization
    Returns:
        List of index dictionaries
    """
    url = f"{api_url}{__INDEXES_ENDPOINT}"
    params = {"limit": __PAGINATION_LIMIT, "offset": 0}
    all_indexes = []

    while True:
        response_data = make_api_request(url, headers, params=params)
        indexes = response_data.get("results", [])
        all_indexes.extend(indexes)

        total = response_data.get("total", 0)
        current_offset = params["offset"]
        fetched_count = len(indexes)

        if current_offset + fetched_count >= total:
            break

        params["offset"] += __PAGINATION_LIMIT

    return all_indexes


def sync_documents_for_index(
    api_url: str,
    headers: Dict[str, str],
    index_uid: str,
    last_sync_timestamp: Optional[int] = None,
):
    """
    Fetch and sync documents from a specific MeiliSearch index.
    Args:
        api_url: The base URL of the MeiliSearch instance
        headers: HTTP headers including authorization
        index_uid: The unique identifier of the index
        last_sync_timestamp: Timestamp in milliseconds for incremental sync filtering
    """
    log.info(f"Syncing documents from index: {index_uid}")

    url = f"{api_url}{__DOCUMENTS_ENDPOINT.format(index_uid=index_uid)}"
    offset = 0
    documents_synced = 0

    while True:
        payload = {"offset": offset, "limit": __PAGINATION_LIMIT}

        if last_sync_timestamp:
            last_updated_seconds = last_sync_timestamp // 1000
            payload["filter"] = f"last_updated >= {last_updated_seconds}"

        response_data = make_api_request(url, headers, method="POST", json_payload=payload)
        documents = response_data.get("results", [])

        for document in documents:
            document_id = extract_document_id(document, index_uid)

            flattened_document = flatten_document(document)
            flattened_document["_index_uid"] = index_uid
            flattened_document["_document_id"] = document_id

            # The 'upsert' operation is used to insert or update data in the destination table.
            # The first argument is the name of the destination table.
            # The second argument is a dictionary containing the record to be upserted.
            op.upsert(table="document", data=flattened_document)
            documents_synced += 1

            if documents_synced % __CHECKPOINT_INTERVAL == 0:
                log.info(f"Processed {documents_synced} documents from index {index_uid}")

        total = response_data.get("total", 0)
        fetched_count = len(documents)

        if offset + fetched_count >= total:
            break

        offset += __PAGINATION_LIMIT

    log.info(f"Synced {documents_synced} documents from index: {index_uid}")


def extract_document_id(document: Dict, index_uid: str) -> str:
    """
    Extract document ID from document data.
    Args:
        document: The document dictionary
        index_uid: The index UID for logging context
    Returns:
        The document ID as a string
    """
    if "id" in document:
        return str(document["id"])
    elif "_id" in document:
        return str(document["_id"])
    else:
        generated_id = str(hash(json.dumps(document, sort_keys=True)))
        log.warning(
            f"Document in index '{index_uid}' has no 'id' or '_id' field. Generated hash-based ID: {generated_id}"
        )
        return generated_id


def flatten_document(document: Dict, prefix: str = "", separator: str = "_") -> Dict:
    """
    Flatten nested document structure for table storage.
    Args:
        document: The document dictionary to flatten
        prefix: Prefix to add to keys
        separator: Separator to use between nested keys
    Returns:
        Flattened dictionary
    """
    flattened = {}

    for key, value in document.items():
        new_key = f"{prefix}{separator}{key}" if prefix else key

        if isinstance(value, dict):
            flattened.update(flatten_document(value, new_key, separator))
        elif isinstance(value, list):
            flattened[new_key] = json.dumps(value)
        else:
            flattened[new_key] = value

    return flattened


# Create the connector object using the schema and update functions
connector = Connector(update=update, schema=schema)

# Check if the script is being run as the main module.
# This is Python's standard entry method allowing your script to be run directly from the command line or IDE 'run' button.
# This is useful for debugging while you write your code. Note this method is not called by Fivetran when executing your connector in production.
# Please test using the Fivetran debug command prior to finalizing and deploying your connector.
if __name__ == "__main__":
    # Open the configuration.json file and load its contents
    with open("configuration.json", "r") as f:
        configuration = json.load(f)

    # Test the connector locally
    connector.debug(configuration=configuration)
