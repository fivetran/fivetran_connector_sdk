from typing import Dict, List
from fivetran_connector_sdk import Logging as log # For enabling Logs in your connector code
from fivetran_connector_sdk import Operations as op # For supporting operations like checkpoint and upsert

from uuid import uuid4 # for generating batch boundaries
import json
from requests_toolbelt.multipart import decoder, encoder # for batch request processing
import datetime


class ODataClient:
    def __init__(self, base_url: str, session, state: Dict = None, timeout: int = 120):
        if not base_url.endswith('/'):
            base_url += '/'
        self.base_url = base_url
        self.session = session
        self.timeout = timeout
        self.batch_requests = []
        self.state = state or {}
        log.info(f"OData client initialized : {self.base_url}")


    def _build_query_options(self, options: Dict) -> str:
        """
        Build OData query options string from a dictionary.
        This method converts a Python dictionary of query options to OData query syntax.
        """
        if not options:
            return ""

        query_parts = []
        self._process_standard_options(options=options, query_parts=query_parts)
        self._process_expand_option(options=options, query_parts=query_parts)
        return '&'.join(query_parts)


    @staticmethod
    def _process_standard_options(options: Dict, query_parts: List[str]) -> None:
        """Process standard OData options and add to query_parts."""
        standard_options = {
            'select': '$select', 'filter': '$filter', 'orderby': '$orderby',
            'top': '$top', 'skip': '$skip', 'count': '$count', 'search': '$search'
        }

        for key, odata_key in standard_options.items():
            if key not in options:
                continue

            value = options[key]
            if key == 'count' and isinstance(value, bool):
                query_parts.append(f"{odata_key}={str(value).lower()}")
            elif isinstance(value, list):
                query_parts.append(f"{odata_key}={','.join(value)}")
            else:
                query_parts.append(f"{odata_key}={value}")


    def _process_expand_option(self, options: Dict, query_parts: List[str]) -> None:
        """
        Process expand option and add to query_parts.
        Handles complex expand scenarios including nested expands and options.
        """
        if 'expand' not in options:
            return

        expand_value = options['expand']
        if isinstance(expand_value, list):
            query_parts.append(f"$expand={','.join(expand_value)}")
        elif isinstance(expand_value, dict):
            expand_parts = self._build_expand_parts(expand_dict=expand_value)
            query_parts.append(f"$expand={expand_parts}")
        else:
            query_parts.append(f"$expand={expand_value}")


    def _build_expand_parts(self, expand_dict: Dict) -> str:
        """Build the expand expression parts from a dictionary."""
        expand_parts = []
        for entity, entity_options in expand_dict.items():
            if isinstance(entity_options, dict):
                nested_options = self._build_nested_options(options=entity_options)
                expand_parts.append(f"{entity}({';'.join(nested_options)})")
            else:
                expand_parts.append(entity)
        return ','.join(expand_parts)


    def _build_nested_options(self, options: Dict) -> List[str]:
        """Build nested options for expand."""
        nested_options = []
        for opt_key, opt_val in options.items():
            if opt_key == 'select' and isinstance(opt_val, list):
                nested_options.append(f"$select={','.join(opt_val)}")
            elif opt_key == 'expand' and isinstance(opt_val, list):
                nested_options.append(f"$expand={','.join(opt_val)}")
            elif opt_key == 'expand' and isinstance(opt_val, dict):
                nested_expand = self._build_expand_parts(expand_dict=opt_val)
                nested_options.append(f"$expand={nested_expand}")
            else:
                nested_options.append(f"${opt_key}={opt_val}")
        return nested_options


    def _build_request_url(self, base_path: str, query_options: Dict = None, relative_url: bool = False) -> str:
        """Build a request URL with the given path and query options."""
        if relative_url:
            url = base_path
        else:
            url = f"{self.base_url}{base_path}"

        if query_options:
            query_string = self._build_query_options(options=query_options)
            if query_string:
                url = f"{url}?{query_string}"

        return url


    def _make_request(self, url: str):
        """Make a GET request to the given URL and return the JSON response."""
        response = self.session.get(url,timeout=self.timeout)
        response.raise_for_status()
        response_data = response.json()
        return response_data


    def _update_state_tracker(self, item: Dict, update_state: Dict) -> None:
        """Updates the state tracker with the highest values from the current record."""
        if not update_state:
            return

        for state_var, column in update_state.items():
            try:
                current_value = item[column]
                if state_var not in self.state or current_value > self.state[state_var]:
                    self.state[state_var] = current_value
            except KeyError:
                log.severe(f"{column} not found in the fetched data")


    def _upsert_formatted_data(self, formatted_data, table, update_state):
        """Upsert the formatted data and update the state tracker."""
        if formatted_data["success"]:
            for item in formatted_data["data"]:
                yield op.upsert(table=table, data=item)
                self._update_state_tracker(item=item, update_state=update_state)

            log.info(f"upserted {formatted_data['count']} records into {table}")
            yield op.checkpoint(self.state)
        else:
            raise RuntimeError(f"Error fetching entity set for table {table}")


    def upsert_entity(self, entity:Dict, additional_headers: Dict = None, handle_pagination: bool = True, state: Dict = None):
        """Fetch an entity set from the OData service and yield records."""
        entity_set = entity.get("entity_set", None)
        query_options = entity.get("query_options", {})
        table = entity.get("table", entity_set)
        update_state = entity.get("update_state", None)

        if not entity_set:
            raise ValueError("entity_set must be specified")

        if state:
            self.state = state

        url = self._build_request_url(base_path=entity_set, query_options=query_options)

        if additional_headers:
            self.session.headers.update(additional_headers)

        try:
            if handle_pagination:
               yield from self._handle_pagination(initial_url=url, table=table, update_state=update_state)
            else:
                response_data = self._make_request(url=url)
                formatted_data = self._standardize_output(response=response_data)
                yield from self._upsert_formatted_data(formatted_data=formatted_data, table=table, update_state=update_state)

            return self.state

        except Exception as e:
            raise ConnectionError(f"Error fetching entity set: {str(e)}")


    def upsert_multiple_entity(self, entity_list: List[Dict], state: Dict = None):
        """
        Fetch data from multiple entity sets and yield records.
        Convenient method to process multiple entity sets sequentially.
        """
        if state:
            self.state = state
        for entity in entity_list:
            yield from self.upsert_entity(entity=entity)
        return self.state


    def _handle_pagination(self, initial_url: str, table: str = None, update_state: Dict = None):
        """
        Handles pagination by following @odata.nextLink until all pages are fetched.
        You can modify this method to handle service-specific pagination formats
        """
        next_link = initial_url

        while next_link:
            current_page = self._make_request(url=next_link)
            formatted_data = self._standardize_output(response=current_page)
            yield from self._upsert_formatted_data(formatted_data=formatted_data, table=table, update_state=update_state)
            next_link = current_page.get("@odata.nextLink") or current_page.get("odata.nextLink")
            if next_link and "http" not in next_link:
                next_link = f"{self.base_url}{next_link}"


    @staticmethod
    def clean_odata_fields(data):
        """
        Recursively clean OData metadata fields (fields starting with '@odata').
        You can modify this method if your service uses different metadata conventions
        or if you want to preserve certain metadata fields.
        """
        if isinstance(data, dict):
            return {k: ODataClient.clean_odata_fields(v) for k, v in data.items()
                    if not (isinstance(k, str) and '@odata' in k)}
        elif isinstance(data, list):
            return [ODataClient.clean_odata_fields(item) for item in data]
        elif isinstance(data, datetime.datetime):
            return data.isoformat()
        else:
            return data


    @staticmethod
    def _standardize_output(response: Dict) -> Dict:
        """
        Standardizes the output format for all OData responses, focusing on business data.
        Creates a consistent format from potentially varied OData responses.
        This is a critical method to customize if your OData service
        has a non-standard response format. You can modify this method to handle service-specific
        response structures or to transform data before upserting.
        """
        result = {
            "data": [],
            "count": 0,
            "success": False
        }

        if "@odata.count" in response:
            result["count"] = response["@odata.count"]

        if "value" in response:
            result["data"] = ODataClient.clean_odata_fields(data=response["value"])
        else:
            entity_data = {k: v for k, v in response.items() if not k.contains('@odata')}
            if entity_data:
                result["data"] = [ODataClient.clean_odata_fields(data=entity_data)]

        if result['count'] == 0:
            result["count"] = len(result["data"])

        result["success"] = True

        return result


    def _paginate_with_batch(self, table, update_state=None, initial_response=None):
        """
        Paginate using $skip and $top parameters.
        Helper method for handling pagination in batch responses.
        """

        if initial_response:
            formatted_data = self._standardize_output(response=initial_response)
            yield from self._upsert_formatted_data(formatted_data=formatted_data, table=table, update_state=update_state)
            next_link = initial_response.get('@odata.nextLink') or initial_response.get('odata.nextLink')

            if next_link:
                next_url = f"{self.base_url}{next_link}"
                yield from self._handle_pagination(initial_url=next_url, table=table, update_state=update_state)
        else:
            raise RuntimeError(f"Error fetching entity set for table {table}")


    def _build_batch_request_body(self, batch_requests, batch_boundary):
        """
        Build the multipart batch request body.
        Creates the MIME multipart request body according to OData batch specification.
        """
        body = []

        for i, request in enumerate(batch_requests):
            body.append(f"--{batch_boundary}")
            body.append("Content-Type: application/http")
            body.append("Content-Transfer-Encoding: binary")
            body.append("")

            entity_set = request['entity_set']
            query_options = request.get('query_options', {})

            relative_url = self._build_request_url(base_path=entity_set, query_options=query_options, relative_url=True)

            body.append(f"GET {relative_url} HTTP/1.1")
            body.append("Accept: application/json")
            body.append("")
            body.append("")

        body.append(f"--{batch_boundary}--")
        return "\r\n".join(body)


    def _process_batch_response(self, response):
        """
        Process multipart batch response using requests_toolbelt.
        Decodes the multipart response and processes each part.
        """
        log.info(f"Processing batch response with {len(self.batch_requests)} requests")
        content_type = response.headers.get('Content-Type', '')

        if 'multipart/mixed' not in content_type:
            log.severe(f"Invalid content type: {content_type}")
            raise ValueError(f"Expected multipart/mixed response, got: {content_type}")

        try:
            multipart_data = decoder.MultipartDecoder(response.content, content_type)
            log.info(f"Successfully decoded multipart response with {len(multipart_data.parts)} parts")

            for i, part in enumerate(multipart_data.parts):
                if i >= len(self.batch_requests):
                    break

                yield from self._process_batch_part(part=part, part_index=i)

        except Exception as e:
            log.severe(f"Failed to process batch response: {str(e)}")
            raise


    def _process_batch_part(self, part, part_index: int):
        """Process an individual part from a multipart response."""
        request_config = self.batch_requests[part_index]
        entity_set = request_config['entity_set']
        table = request_config.get('table', entity_set)
        update_state = request_config.get('update_state')

        log.info(f"Processing part {part_index + 1} for entity: {entity_set}")

        if not part.content:
            log.severe(f"Empty content in part {part_index}")
            return

        try:
            # Extract JSON data from the part
            json_content = self._extract_json_from_part(part=part, part_index=part_index)
            if not json_content:
                return

            # Parse the JSON content
            response_data = json.loads(json_content)
            record_count = len(response_data.get('value', []))
            log.info(f"Part {part_index} contains {record_count} records")

            # Process with pagination
            yield from self._paginate_with_batch(table=table, update_state=update_state, initial_response=response_data)

        except Exception as e:
            raise RuntimeError(f"Error processing part {part_index}: {str(e)}")


    def _extract_json_from_part(self, part, part_index: int) -> str:
        """Extract JSON content from a multipart part, handling nested HTTP structure."""
        try:
            content_str = part.content.decode('utf-8')

            body_start = ODataClient._find_header_body_separator(content=content_str)
            if body_start == -1:
                raise RuntimeError(f"Cannot find HTTP headers/body separator in part {part_index}")

            http_body = content_str[body_start:].strip()
            json_start = ODataClient._find_header_body_separator(content=http_body)

            if json_start != -1:
                return http_body[json_start:].strip()
            else:
                return http_body.strip()

        except UnicodeDecodeError:
            log.severe(f"Unicode decode error in part {part_index}")
            raise


    @staticmethod
    def _find_header_body_separator(content: str) -> int:
        """Find the separator between headers and body in HTTP content."""

        separator_pos = content.find('\r\n\r\n')
        if separator_pos != -1:
            return separator_pos + 4

        separator_pos = content.find('\n\n')
        if separator_pos != -1:
            return separator_pos + 2

        return -1


    def add_batch(self, entity: Dict):
        """
        Add a batch request to the batch queue.
        Use this method to queue multiple requests before executing them
        in a single batch with upsert_batch().
        """
        entity_set = entity.get("entity_set", None)

        if not entity_set:
            raise ValueError("entity_set must be specified")

        batch_request = {
            'entity_set': entity_set,
            'query_options':entity.get('query_options', {}),
            'table':  entity.get('table', entity_set),
            'update_state': entity.get('update_state', None)
        }

        self.batch_requests.append(batch_request)
        return self  # Enable method chaining


    def upsert_batch(self, state:Dict = None):
        """
        Execute a batch request with multiple parts.
        Sends all queued batch requests in a single HTTP request and processes the responses.
        """
        if not self.batch_requests:
            log.warning("No batch requests to execute")
            return self.state

        if state:
            self.state = state

        batch_boundary = f"batch_{uuid4().hex}"

        headers = {
            'Content-Type': f'multipart/mixed; boundary={batch_boundary}',
            'Accept': 'multipart/mixed',
            'Content-Transfer-Encoding': 'binary'
        }

        request_body = self._build_batch_request_body(batch_requests=
            self.batch_requests, batch_boundary=batch_boundary)

        batch_url = f"{self.base_url}$batch"
        response = self.session.post(
            batch_url,
            headers=headers,
            data=request_body,
            timeout=self.timeout
        )
        response.raise_for_status()

        yield from self._process_batch_response(response=response)

        self.batch_requests = []
        return self.state